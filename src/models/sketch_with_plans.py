# sketch_with_plans.py

"""
Usage:
    PYTHONPATH=. python src/models/sketch_with_plans.py --instruction_set toplevel_leaves
    PYTHONPATH=. python src/models/sketch_with_plans.py --dataset ndjson --instruction_set stack
    PYTHONPATH=. python src/models/sketch_with_plans.py --dataset ndjson --instruction_set toplevel --cond_instructions match
"""

from functools import partial
import os
from os.path import abspath

import numpy as np
import torch
from torch import nn, optim
import torch.nn.functional as F
from torch.utils.data import DataLoader

from src import utils
from src.models.core import nn_utils
from src.models.core.train_nn import RUNS_PATH, TrainNN
from src.models.base.instruction_models import (
    ProgressionPairDataset,
    SketchWithPlansConditionEntireDrawingDataset,
    SketchWithPlansConditionSegmentsDataset,
    InstructionEncoderTransformer,
    LABELED_PROGRESSION_PAIRS_IDX2TOKEN_PATH
)
from src.models.base.stroke_models import (
    SketchRNNDecoderGMM
)
from src.models.sketch_rnn import SketchRNNModel
from src.models.sketch_rnn import HParams as SketchRNNHParams

USE_CUDA = torch.cuda.is_available()

class HParams(SketchRNNHParams):
    def __init__(self):
        super().__init__()

        # Data
        self.dataset = 'ndjson'   # 'progressionpair' or 'ndjson'
        self.max_per_category = 250

        # Model
        self.instruction_set = 'toplevel'  # 'toplevel_leaves',  'stack'
        self.cond_instructions = 'match'  # 'initdec', 'decinputs', 'match'
        self.enc_dim = 512
        self.dec_dim = 512  # as is implemented right now, enc_dim == dec_dim when cond_instructions==initdec

        self.lr = 0.0005

class SketchRNNWithPlans(SketchRNNModel):
    """"
    SketchRNN that also encodes and conditions on top-level instruction (i.e. instruction for entire
    drawing) generated by an instruction generation model.
    """
    def __init__(self, hp, save_dir):
        super().__init__(hp, save_dir)

        self.end_epoch_loader = None  # TODO: not generating yet, need to refactor that

        if hp.instruction_set == 'stack':
            hp.cond_instructions == 'decinputs'

        # Model
        vocab_size = len(utils.load_file(LABELED_PROGRESSION_PAIRS_IDX2TOKEN_PATH))
        self.text_embedding = nn.Embedding(vocab_size, hp.enc_dim)
        self.enc = InstructionEncoderTransformer(hp.enc_dim, hp.enc_num_layers, hp.dropout, use_categories=False)  # TODO: should this be a hparam
        dec_input_dim = (5 + hp.enc_dim) if (hp.cond_instructions == 'decinputs') else 5  # dec_inputs
        self.dec = SketchRNNDecoderGMM(dec_input_dim, hp.dec_dim, hp.M)  # Method 1 (see one_forward_pass, i.e. decinputs)

        self.models.extend([self.text_embedding, self.enc, self.dec])
        if USE_CUDA:
            for model in self.models:
                model.cuda()

        self.optimizers.append(optim.Adam(self.parameters(), hp.lr))

    def get_data_loader(self, dataset_split, batch_size, categories, max_len, max_per_category, shuffle):
        """
        Args:
            dataset_split (str): 'train', 'valid', 'test'
            batch_size (int)
            categories (str
            shuffle (bool)
        """
        if self.hp.instruction_set in ['toplevel', 'toplevel_leaves']:
            ds = SketchWithPlansConditionEntireDrawingDataset(dataset=hp.dataset,
                                                              max_len=max_len,
                                                              max_per_category=hp.max_per_category,
                                                              dataset_split=dataset_split,
                                                              instruction_set=self.hp.instruction_set)
            loader = DataLoader(ds, batch_size=batch_size, shuffle=shuffle,
                                collate_fn=ProgressionPairDataset.collate_fn)
        elif self.hp.instruction_set == 'stack':
            ds = SketchWithPlansConditionSegmentsDataset(dataset=hp.dataset,
                                                         max_len=max_len,
                                                         max_per_category=hp.max_per_category,
                                                         dataset_split=dataset_split,
                                                         instruction_set=self.hp.instruction_set)
            loader = DataLoader(ds, batch_size=batch_size, shuffle=shuffle,
                                collate_fn=partial(SketchWithPlansConditionSegmentsDataset.collate_fn, token2idx=ds.token2idx))
        return loader

    def preprocess_batch_from_data_loader(self, batch):
        """
        Convert tensors to cuda and convert to [len, bsz, ...] instead of [bsz, len, ...]
        """
        preprocessed = []
        for item in batch:
            if type(item) == torch.Tensor:
                item = nn_utils.move_to_cuda(item)
                if item.dim() > 1:
                    item.transpose_(0, 1)
            preprocessed.append(item)
        return preprocessed

    def one_forward_pass(self, batch):
        """
        Return loss and other items of interest for one forward pass

        Args:
            batch: tuple from DataLoaders

        Returns:
            dict where 'loss': float Tensor must exist
        """
        strokes, stroke_lens, texts, text_lens, text_indices, cats, cats_idx, urls = batch

        # Create base inputs to decoder
        _, bsz, _ = strokes.size()
        sos = torch.stack([torch.Tensor([0, 0, 1, 0, 0])] * bsz).unsqueeze(0)  # start of sequence
        sos = nn_utils.move_to_cuda(sos)
        dec_inputs = torch.cat([sos, strokes], dim=0)  # add sos at the begining of the strokes; [max_len + 1, bsz, 5]

        #
        # Encode instructions, decode
        #
        if self.hp.instruction_set == 'stack':
            # text_indices: [max_seq_len, bsz, max_instruction_len], # text_lens: [max_seq_len, bsz]

            if hp.cond_instructions == 'decinputs':
                # decoder is conditioned on stack of instructions at every time step
                max_seq_len = strokes.size(0)
                hidden = []
                for i in range(max_seq_len):
                    drawing_text_indices = text_indices[i].t()               # [max_instruction_len, bsz]
                    drawing_text_lens = text_lens[i].cpu().numpy().tolist()  # [bsz]
                    # however, max_instruction_len is max across all drawings. Transformer encoder module
                    # must have input length equal to maximum length. Thus, do the following:
                    drawing_text_indices = drawing_text_indices[:max(drawing_text_lens),:]
                    hidden_i = self.enc(drawing_text_indices,
                                        drawing_text_lens,
                                        self.text_embedding,
                                        category_embedding=None, categories=cats_idx)  # [bsz, dim]
                    hidden.append(hidden_i)
                hidden = torch.stack(hidden)  # [max_seq_len, bsz, dim]

                # Concat stack of instructions (which occur at every time step) to dec_inputs
                hidden = torch.cat([hidden[0].unsqueeze(0), hidden], dim=0)  # sos adds a timestep
                dec_inputs = torch.cat([dec_inputs, hidden], dim=2)  # [max_len + 1, bsz, 5 + dim]
                pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q, _, _ = self.dec(dec_inputs, output_all=True)

            elif hp.cond_instructions == 'match':
                # decoder's hidden states are "matched" with language representations

                outputs, pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q, _, _ = self.dec(dec_inputs, output_all=True)

                import pdb; pdb.set_trace()  # outputs: [max_seq_len, bsz, dim]
                for i in range(bsz):
                    penups = np.where(strokes[:,i,:].cpu().numpy()[0][:,3] == 1)[0].tolist()
                    penups = [0] + penups

                    for j in range(len(penups) - 1):
                        start_idx = penups[j]
                        end_idx = penups[j+1]
                        seg_outputs = outputs[start_idx:end_idx+1, i, :]  # [seg_len, dim]


                # for i in range(bsz):
                #     penups = np.where(strokes.cpu().numpy()[0][:,3] == 1)[0].tolist()
                #     # TODO: why are text_indices different within the same segment... that seems like a bug?
                #     for

                import pdb; pdb.set_trace()

        elif self.hp.instruction_set in ['toplevel', 'toplevel_leaves']:
            # Encode instructions
            # text_indices: [len, bsz], text_lens: [bsz]
            hidden = self.enc(text_indices, text_lens, self.text_embedding,
                              category_embedding=None, categories=cats_idx)  # [bsz, dim]

            # Method 1: concatenate instruction embedding to every time step
            if self.hp.cond_instructions == 'decinputs':
                hidden = hidden.unsqueeze(0)  #  [1, bsz, dim]
                hidden = hidden.repeat(dec_inputs.size(0), 1, 1)  # [max_len + 1, bsz, dim]
                dec_inputs = torch.cat([dec_inputs, hidden], dim=2)  # [max_len + 1, bsz, 5 + dim]
                outputs, pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q, _, _ = self.dec(dec_inputs, output_all=True)

            # Method 2: initialize decoder's hidden state with instruction embedding
            elif self.hp.cond_instructions == 'initdec':
                hidden = hidden.unsqueeze(0)  #  [1, bsz, dim]
                hidden_cell = (hidden, hidden.clone())
                outputs, pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q, _, _ = self.dec(dec_inputs, output_all=True, hidden_cell=hidden_cell)


            elif self.hp.cond_instructions == 'match':
                outputs, pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy, q, _, _ = self.dec(dec_inputs, output_all=True)  # outputs: [max_seq_len, bsz, dim]
                outputs = outputs.mean(dim=0)  # [bsz, dim]

                # triplet loss
                pos = (outputs - hidden) ** 2  # [bsz]
                hidden_shuffled = hidden[torch.randperm(bsz), :]  # [bsz, dim]
                neg = (outputs - hidden_shuffled) ** 2  # [bsz]
                loss_match = (pos - neg).mean()+ torch.tensor(0.1).to(pos.device)  # positive - negative + alpha
                loss_match = max(torch.tensor(0.0), loss_match)


        #
        # Calculate losses
        #
        mask, dx, dy, p = self.dec.make_target(strokes, stroke_lens, self.hp.M)

        loss_R = self.dec.reconstruction_loss(mask,
                                            dx, dy, p,
                                            pi, mu_x, mu_y, sigma_x, sigma_y, rho_xy,
                                            q)
        if hp.cond_instructions == 'match':
            loss = loss_R + loss_match
            result = {'loss': loss, 'loss_R': loss_R, 'loss_match': loss_match}
        else:
            result = {'loss': loss_R, 'loss_R': loss_R}

        if ((loss != loss).any() or (loss == float('inf')).any() or (loss == float('-inf')).any()):
            raise Exception('Nan in SketchRNnDecoderGMMOnly forward pass')

        return result

    def generate_and_save(self, data_loader, epoch, n_gens=1, outputs_path=None):
        # TODO: need to overwrite this. SketchRNN's generate_and_save() unpacks a batch from
        # a dataset that returns 4 values. The SketchWithPlansDataset returns 8 values. The
        # encoding of the instructions is different too. Need to refactor that bit to work
        # with both.
        pass

if __name__ == "__main__":
    hp = HParams()
    hp, run_name, parser = utils.create_argparse_and_update_hp(hp)
    parser.add_argument('--groupname', default='debug', help='name of subdir to save runs')
    opt = parser.parse_args()
    nn_utils.setup_seeds()

    save_dir = os.path.join(RUNS_PATH, 'sketchwplans', opt.groupname, run_name)
    utils.save_run_data(save_dir, hp)

    model = None
    model = SketchRNNWithPlans(hp, save_dir)
    model.train_loop()
